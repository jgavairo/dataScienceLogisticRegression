#!/usr/bin/env python3
"""
logreg_evaluate.py - Evaluates the logistic regression model accuracy

This script compares the model predictions with the true values
and calculates various performance metrics.

Usage:
    python3 logreg_evaluate.py

Input:
    - datasets/dataset_truth.csv (true houses for test dataset)
    - output/houses.csv (predictions generated by logreg_predict.py)

Output:
    - Displays performance metrics
    - Confusion matrix
    - Accuracy per house
"""

import pandas as pd
import sys


def load_predictions(predictions_path='output/houses.csv'):
    """
    Load predictions generated by logreg_predict.py
    
    Args:
        predictions_path: Path to the predictions file
        
    Returns:
        DataFrame with predictions
    """
    try:
        predictions = pd.read_csv(predictions_path)
        print(f"‚úì Predictions loaded: {len(predictions)} examples")
        return predictions
    except FileNotFoundError:
        print(f"‚ùå Error: File {predictions_path} not found")
        print("   Run first: python3 logreg_predict.py")
        sys.exit(1)


def load_true_labels(dataset_path='datasets/dataset_truth.csv'):
    """
    Load true labels from dataset_truth.csv
    
    Args:
        dataset_path: Path to the ground truth file
        
    Returns:
        DataFrame with Index and Hogwarts House
    """
    try:
        true_labels = pd.read_csv(dataset_path)
        
        # Check that required columns are present
        if 'Index' not in true_labels.columns or 'Hogwarts House' not in true_labels.columns:
            print(f"‚ùå Error: File must contain columns 'Index' and 'Hogwarts House'")
            sys.exit(1)
        
        # Remove rows without house (if any)
        true_labels = true_labels.dropna(subset=['Hogwarts House'])
        
        print(f"‚úì True labels loaded: {len(true_labels)} examples")
        return true_labels
    except FileNotFoundError:
        print(f"‚ùå Error: File {dataset_path} not found")
        print("   This file contains the true houses for the test dataset")
        sys.exit(1)


def calculate_accuracy(predictions_df, true_labels_df):
    """
    Calculate overall and per-class accuracy
    
    Args:
        predictions_df: DataFrame with columns [Index, Hogwarts House]
        true_labels_df: DataFrame with columns [Index, Hogwarts House]
        
    Returns:
        dict: Dictionary with metrics
    """
    # Merge DataFrames on Index
    merged = predictions_df.merge(
        true_labels_df, 
        on='Index', 
        suffixes=('_pred', '_true')
    )
    
    if len(merged) == 0:
        print("‚ùå Error: No match between predictions and true values")
        sys.exit(1)
    
    # Calculate overall accuracy
    correct = (merged['Hogwarts House_pred'] == merged['Hogwarts House_true']).sum()
    total = len(merged)
    accuracy = correct / total
    
    # List of houses
    houses = ['Gryffindor', 'Hufflepuff', 'Ravenclaw', 'Slytherin']
    
    # Calculate per-house metrics (Precision, Recall, F1-Score)
    metrics = {}
    
    for house in houses:
        # True Positives: predicted as this house AND actually this house
        tp = ((merged['Hogwarts House_pred'] == house) & 
              (merged['Hogwarts House_true'] == house)).sum()
        
        # False Positives: predicted as this house BUT not actually this house
        fp = ((merged['Hogwarts House_pred'] == house) & 
              (merged['Hogwarts House_true'] != house)).sum()
        
        # False Negatives: not predicted as this house BUT actually this house
        fn = ((merged['Hogwarts House_pred'] != house) & 
              (merged['Hogwarts House_true'] == house)).sum()
        
        # True Negatives: not predicted as this house AND not actually this house
        tn = ((merged['Hogwarts House_pred'] != house) & 
              (merged['Hogwarts House_true'] != house)).sum()
        
        # Precision: TP / (TP + FP)
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        
        # Recall: TP / (TP + FN)
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        # F1-Score: harmonic mean of precision and recall
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        metrics[house] = {
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'support': tp + fn  # Actual number of examples for this house
        }
    
    return {
        'accuracy': accuracy,
        'correct': correct,
        'total': total,
        'per_class': metrics,
        'merged_df': merged
    }


def create_confusion_matrix(merged_df):
    """
    Create a confusion matrix
    
    Args:
        merged_df: DataFrame with columns Hogwarts House_pred and Hogwarts House_true
        
    Returns:
        DataFrame: Confusion matrix
    """
    houses = ['Gryffindor', 'Hufflepuff', 'Ravenclaw', 'Slytherin']
    
    # Initialize the matrix
    matrix = pd.DataFrame(0, index=houses, columns=houses)
    
    # Fill the matrix
    for _, row in merged_df.iterrows():
        true_house = row['Hogwarts House_true']
        pred_house = row['Hogwarts House_pred']
        matrix.loc[true_house, pred_house] += 1
    
    return matrix


def print_results(results):
    """
    Display results in a formatted way
    
    Args:
        results: Dictionary returned by calculate_accuracy()
    """
    print("\n" + "="*80)
    print("üìä LOGISTIC REGRESSION MODEL EVALUATION")
    print("="*80)
    
    # Overall accuracy
    print(f"\n‚ú® OVERALL ACCURACY: {results['accuracy']:.2%}")
    print(f"   ‚Üí {results['correct']} / {results['total']} correct predictions")
    
    # Per-class metrics
    print("\n" + "-"*80)
    print("üìà METRICS PER HOUSE:")
    print("-"*80)
    print(f"{'House':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}")
    print("-"*80)
    
    for house, metrics in results['per_class'].items():
        print(f"{house:<15} "
              f"{metrics['precision']:.2%}        "
              f"{metrics['recall']:.2%}      "
              f"{metrics['f1_score']:.2%}      "
              f"{metrics['support']:<10}")
    
    # Average metrics
    avg_precision = sum(m['precision'] for m in results['per_class'].values()) / 4
    avg_recall = sum(m['recall'] for m in results['per_class'].values()) / 4
    avg_f1 = sum(m['f1_score'] for m in results['per_class'].values()) / 4
    
    print("-"*80)
    print(f"{'Average':<15} "
          f"{avg_precision:.2%}        "
          f"{avg_recall:.2%}      "
          f"{avg_f1:.2%}")
    
    # Confusion matrix
    print("\n" + "-"*80)
    print("üéØ CONFUSION MATRIX:")
    print("-"*80)
    print("       Rows = True values | Columns = Predictions")
    print()
    
    confusion = create_confusion_matrix(results['merged_df'])
    
    # Display the matrix with better formatting
    header = "True \\ Pred"
    print(f"{header:<15}", end='')
    for house in confusion.columns:
        print(f"{house[:10]:>12}", end='')
    print()
    print("-"*80)
    
    for house in confusion.index:
        print(f"{house:<15}", end='')
        for pred_house in confusion.columns:
            value = confusion.loc[house, pred_house]
            print(f"{int(value):>12}", end='')
        print()
    
    print("="*80)
    
    # Interpretation
    print("\nüí° INTERPRETATION:")
    if results['accuracy'] >= 0.98:
        print("   üåü EXCELLENT! Your model has very high accuracy.")
    elif results['accuracy'] >= 0.95:
        print("   ‚úÖ VERY GOOD! Your model performs well.")
    elif results['accuracy'] >= 0.90:
        print("   üëç GOOD! Your model is decent but could be improved.")
    elif results['accuracy'] >= 0.80:
        print("   ‚ö†Ô∏è  ACCEPTABLE. Consider adjusting hyperparameters.")
    else:
        print("   ‚ùå WEAK. The model needs significant improvements.")
    
    # Tips based on errors
    worst_house = min(results['per_class'].items(), key=lambda x: x[1]['f1_score'])
    best_house = max(results['per_class'].items(), key=lambda x: x[1]['f1_score'])
    
    print(f"\n   üìâ Most difficult house: {worst_house[0]} (F1={worst_house[1]['f1_score']:.2%})")
    print(f"   üìà Best predicted house: {best_house[0]} (F1={best_house[1]['f1_score']:.2%})")
    
    print("\n" + "="*80)


def save_detailed_report(results, output_path='output/evaluation_report.txt'):
    """
    Save a detailed report to a file
    
    Args:
        results: Dictionary returned by calculate_accuracy()
        output_path: Path to the output file
    """
    with open(output_path, 'w') as f:
        f.write("="*80 + "\n")
        f.write("EVALUATION REPORT - Multi-Class Logistic Regression\n")
        f.write("="*80 + "\n\n")
        
        f.write(f"Overall Accuracy: {results['accuracy']:.4f} ({results['accuracy']:.2%})\n")
        f.write(f"Correct predictions: {results['correct']} / {results['total']}\n\n")
        
        f.write("-"*80 + "\n")
        f.write("DETAILED METRICS PER HOUSE\n")
        f.write("-"*80 + "\n\n")
        
        for house, metrics in results['per_class'].items():
            f.write(f"{house}:\n")
            f.write(f"  - Precision: {metrics['precision']:.4f}\n")
            f.write(f"  - Recall:    {metrics['recall']:.4f}\n")
            f.write(f"  - F1-Score:  {metrics['f1_score']:.4f}\n")
            f.write(f"  - Support:   {metrics['support']}\n\n")
        
        f.write("-"*80 + "\n")
        f.write("CONFUSION MATRIX\n")
        f.write("-"*80 + "\n\n")
        
        confusion = create_confusion_matrix(results['merged_df'])
        f.write(confusion.to_string())
        f.write("\n\n" + "="*80 + "\n")
    
    print(f"\nüìÑ Detailed report saved to: {output_path}")


def main():
    """
    Main function
    """
    print("\nüîç Evaluating logistic regression model...\n")
    
    # Load data
    predictions = load_predictions()
    true_labels = load_true_labels()
    
    # Calculate metrics
    results = calculate_accuracy(predictions, true_labels)
    
    # Display results
    print_results(results)
    
    # Save report
    save_detailed_report(results)
    
    print("\n‚úÖ Evaluation complete!\n")


if __name__ == "__main__":
    main()
